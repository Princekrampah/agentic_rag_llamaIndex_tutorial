{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load lora_paper.pdf documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# chunk_size of 1024 is a good default value\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "# Create nodes from documents\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# LLM model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "# embedding model\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "# summary index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "# vector store index\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "\n",
    "# summary query engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "# vector query engine\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to the LoRA paper.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the the LoRA paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[summary_tool, vector_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is the LoRA and why it's being used. Are esiting solutions not enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"LoRA and its use cases\"}\n",
      "=== Function Output ===\n",
      "LoRA can be applied to all weight matrices and training all biases, allowing for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. It converges to training the original model as the number of trainable parameters increases. Additionally, LoRA can be used in downstream tasks without additional inference latency by computing and storing specific values, enabling quick switching between tasks with minimal memory overhead.\n",
      "=== LLM Response ===\n",
      "LoRA, which stands for Low-Rank Adaptation, is a technique that can be applied to all weight matrices and training all biases. It allows for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. \n",
      "\n",
      "LoRA is used to converge to training the original model as the number of trainable parameters increases. It can also be applied in downstream tasks without additional inference latency by computing and storing specific values, enabling quick switching between tasks with minimal memory overhead.\n",
      "\n",
      "Existing solutions may not provide the same level of expressiveness and efficiency that LoRA offers, making it a valuable technique for various machine learning applications.\n",
      "assistant: LoRA, which stands for Low-Rank Adaptation, is a technique that can be applied to all weight matrices and training all biases. It allows for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. \n",
      "\n",
      "LoRA is used to converge to training the original model as the number of trainable parameters increases. It can also be applied in downstream tasks without additional inference latency by computing and storing specific values, enabling quick switching between tasks with minimal memory overhead.\n",
      "\n",
      "Existing solutions may not provide the same level of expressiveness and efficiency that LoRA offers, making it a valuable technique for various machine learning applications.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain to me what is the LoRA and why it's being used. Are esiting solutions not enough?\",\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is the LoRA and why it's being used. Are esiting solutions not enough?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"LoRA and its use cases\"}\n",
      "=== Function Output ===\n",
      "LoRA can be applied to all weight matrices and training all biases, allowing for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. It converges to training the original model as the number of trainable parameters increases. When deployed in production, LoRA does not introduce additional inference latency, as computations can be performed as usual. Switching to another downstream task involves quick operations with minimal memory overhead. Additionally, LoRA has been evaluated against various models such as DeBERTa XXL and GPT-2 medium/large, showcasing its competitive performance in both NLU and NLG tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Challenges with existing solutions in machine learning\"}\n",
      "=== Function Output ===\n",
      "The challenges with existing solutions in machine learning include the need for fine-tuning models to improve performance drastically compared to few-shot learning, especially when dealing with datasets that are large and small. This indicates that while few-shot learning or prompt engineering can be advantageous with limited training samples, fine-tuning becomes essential when a few thousand or more training examples are available for performance-sensitive applications.\n",
      "=== LLM Response ===\n",
      "LoRA, or Low-Rank Adaptation, can be applied to all weight matrices and training all biases, allowing for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. It converges to training the original model as the number of trainable parameters increases. When deployed in production, LoRA does not introduce additional inference latency, as computations can be performed as usual. Switching to another downstream task involves quick operations with minimal memory overhead. Additionally, LoRA has been evaluated against various models such as DeBERTa XXL and GPT-2 medium/large, showcasing its competitive performance in both Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks.\n",
      "\n",
      "Existing solutions in machine learning face challenges such as the need for fine-tuning models to improve performance drastically compared to few-shot learning, especially when dealing with datasets that are large and small. This indicates that while few-shot learning or prompt engineering can be advantageous with limited training samples, fine-tuning becomes essential when a few thousand or more training examples are available for performance-sensitive applications.\n",
      "assistant: LoRA, or Low-Rank Adaptation, can be applied to all weight matrices and training all biases, allowing for the recovery of the expressiveness of full fine-tuning by setting the LoRA rank to the rank of the pre-trained weight matrices. It converges to training the original model as the number of trainable parameters increases. When deployed in production, LoRA does not introduce additional inference latency, as computations can be performed as usual. Switching to another downstream task involves quick operations with minimal memory overhead. Additionally, LoRA has been evaluated against various models such as DeBERTa XXL and GPT-2 medium/large, showcasing its competitive performance in both Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks.\n",
      "\n",
      "Existing solutions in machine learning face challenges such as the need for fine-tuning models to improve performance drastically compared to few-shot learning, especially when dealing with datasets that are large and small. This indicates that while few-shot learning or prompt engineering can be advantageous with limited training samples, fine-tuning becomes essential when a few thousand or more training examples are available for performance-sensitive applications.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Explain to me what is the LoRA and why it's being used. Are esiting solutions not enough?\",\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lower Level Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tools=[summary_tool, vector_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Explain to me what is the LoRA and why it's being used.\"\n",
    "    \"Are existing solutions not enough?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the LoRA paper about?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"summary of the LoRA paper\"}\n",
      "=== Function Output ===\n",
      "LoRA is a method that allows for efficient adaptation of pre-trained models to new tasks without requiring full-rank gradient updates. By setting the LoRA rank appropriately, it can approximate the expressiveness of full fine-tuning while avoiding additional inference latency. The paper demonstrates that LoRA can perform competitively with a small rank, suggesting that a low-rank adaptation matrix may be sufficient for effective adaptation. However, the effectiveness of a small rank may vary depending on the task or dataset, as highlighted by the example of different languages requiring different adaptation approaches.\n"
     ]
    }
   ],
   "source": [
    "step_output =  agent.run_step(\n",
    "    task.task_id, input=\"What is the LoRA paper about?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM Response ===\n",
      "The LoRA paper introduces a method for efficiently adapting pre-trained models to new tasks without the need for full-rank gradient updates. By setting the LoRA rank appropriately, it can approximate the expressiveness of full fine-tuning while minimizing additional inference latency. The paper shows that LoRA can achieve competitive performance even with a small rank, indicating that a low-rank adaptation matrix may be effective for adaptation. However, the effectiveness of a small rank may vary based on the task or dataset, as demonstrated by the need for different adaptation approaches for different languages.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e31bf7cf-7ba4-4aec-a289-c7eefec4451d\n",
      "LoRA is a method that allows for efficient adaptation of pre-trained models to new tasks without requiring full-rank gradient updates. By setting the LoRA rank appropriately, it can approximate the expressiveness of full fine-tuning while avoiding additional inference latency. The paper demonstrates that LoRA can perform competitively with a small rank, suggesting that a low-rank adaptation matrix may be sufficient for effective adaptation. However, the effectiveness of a small rank may vary depending on the task or dataset, as highlighted by the example of different languages requiring different adaptation approaches.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(task.task_id)\n",
    "\n",
    "if len(completed_steps) > 0:\n",
    "    print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "\n",
    "if len(upcoming_steps) > 0:\n",
    "    print(upcoming_steps[0].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from an empty deque",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(step_output\u001b[38;5;241m.\u001b[39mis_last)\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:478\u001b[0m, in \u001b[0;36mAgentRunner.run_step\u001b[0;34m(self, task_id, input, step, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m step \u001b[38;5;241m=\u001b[39m validate_step_from_args(task_id, \u001b[38;5;28minput\u001b[39m, step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:400\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_task(task_id)\n\u001b[1;32m    399\u001b[0m step_queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_step_queue(task_id)\n\u001b[0;32m--> 400\u001b[0m step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mstep_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     step\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: LoRA is a method that allows for efficient adaptation of pre-trained models to new tasks without requiring full-rank updates to weight matrices. It enables quick adaptation to downstream tasks with minimal additional inference latency and memory overhead. Existing solutions in NLP have limitations in handling tasks with limited training samples efficiently, and LoRA addresses this by providing a more computationally efficient adaptation method. While few-shot learning and prompt engineering can be beneficial, they may not always offer optimal performance compared to fine-tuning with a larger set of training examples. Therefore, LoRA's approach fills a gap in the existing solutions by providing a more efficient way to adapt pre-trained models to new tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.finalize_response(task.task_id)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from an empty deque",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step_output \u001b[38;5;241m=\u001b[39m  \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the LoRA paper about?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:478\u001b[0m, in \u001b[0;36mAgentRunner.run_step\u001b[0;34m(self, task_id, input, step, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m step \u001b[38;5;241m=\u001b[39m validate_step_from_args(task_id, \u001b[38;5;28minput\u001b[39m, step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m~/Desktop/content/tutorials/agentic_rag/basics/.venv/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:400\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_task(task_id)\n\u001b[1;32m    399\u001b[0m step_queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_step_queue(task_id)\n\u001b[0;32m--> 400\u001b[0m step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mstep_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     step\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
