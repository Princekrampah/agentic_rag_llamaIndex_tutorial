{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic RAG With Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [\n",
    "    \"./datasets/lora_paper.pdf\",\n",
    "    \"./datasets/longlora_efficient_fine_tuning.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_doc_tools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ./datasets/lora_paper.pdf tool\n",
      "Creating ./datasets/longlora_efficient_fine_tuning.pdf tool\n"
     ]
    }
   ],
   "source": [
    "paper_to_tools_dict = {}\n",
    "\n",
    "\n",
    "for paper in papers:\n",
    "    print(f\"Creating {paper} tool\")\n",
    "    path = Path(paper)\n",
    "    vector_tool, summary_tool = await create_doc_tools(doc_name=path.stem, document_fp=path)\n",
    "    paper_to_tools_dict[path.stem] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lora_paper': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x7e37acfd20b0>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7e37acfd2500>],\n",
       " 'longlora_efficient_fine_tuning': [<llama_index.core.tools.query_engine.QueryEngineTool at 0x7e37ab5e0b80>,\n",
       "  <llama_index.core.tools.query_engine.QueryEngineTool at 0x7e37ab5e0c40>]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_to_tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.query_engine.QueryEngineTool object at 0x7e37acfd20b0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7e37acfd2500>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7e37ab5e0b80>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7e37ab5e0c40>]\n"
     ]
    }
   ],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[Path(paper).stem]]\n",
    "print(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain to me what is the Lora and why it's being used.Explain to me what is LongLoRA and why it's being used.Compare and contract LongLoRA and Lora.\n",
      "=== Calling Function ===\n",
      "Calling function: lora_paper_summary_query_engine_tool with args: {\"input\": \"Explain what is Lora and why it's being used.\"}\n",
      "=== Function Output ===\n",
      "LoRA, or Low-Rank Adaptation, is a method utilized for adapting large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights. This approach significantly reduces the number of trainable parameters for downstream tasks, making it more efficient and cost-effective to fine-tune large models like GPT-3 with a high number of parameters. LoRA allows for the efficient adaptation of models to new tasks while preserving the learned knowledge from pre-training, ultimately improving model performance on various tasks with limited training data.\n",
      "=== Calling Function ===\n",
      "Calling function: longlora_efficient_fine_tuning_summary_query_engine_tool with args: {\"input\": \"Explain what is LongLoRA and why it's being used.\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is a framework designed to efficiently extend the context length of large language models (LLMs) through supervised fine-tuning. It incorporates shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns effectively. LongLoRA aims to minimize computational costs and reduce training time while enabling pre-trained LLMs to handle significantly larger context lengths. By combining improved LoRA with S2-Attn, LongLoRA facilitates the fine-tuning of LLMs for long context extension, enhancing their question-answering abilities on long-context benchmarks. The framework also integrates components like Flash-Attention2 and DeepSpeed to optimize training efficiency, making it a valuable tool for extending model capabilities efficiently.\n",
      "=== LLM Response ===\n",
      "**Lora:**\n",
      "Lora, or Low-Rank Adaptation, is a method used for adapting large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights. This approach significantly reduces the number of trainable parameters for downstream tasks, making it more efficient and cost-effective to fine-tune large models like GPT-3 with a high number of parameters. Lora allows for the efficient adaptation of models to new tasks while preserving the learned knowledge from pre-training, ultimately improving model performance on various tasks with limited training data.\n",
      "\n",
      "**LongLoRA:**\n",
      "LongLoRA is a framework designed to efficiently extend the context length of large language models (LLMs) through supervised fine-tuning. It incorporates shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns effectively. LongLoRA aims to minimize computational costs and reduce training time while enabling pre-trained LLMs to handle significantly larger context lengths. By combining improved LoRA with S2-Attn, LongLoRA facilitates the fine-tuning of LLMs for long context extension, enhancing their question-answering abilities on long-context benchmarks. The framework also integrates components like Flash-Attention2 and DeepSpeed to optimize training efficiency, making it a valuable tool for extending model capabilities efficiently.\n",
      "\n",
      "**Comparison and Contrast:**\n",
      "- **Purpose:** Lora is focused on adapting pre-trained language models to specific tasks efficiently, while LongLoRA aims to extend the context length of large language models through supervised fine-tuning.\n",
      "- **Methodology:** Lora utilizes trainable rank decomposition matrices in each layer of the Transformer architecture, while LongLoRA incorporates shifted sparse attention (S2-Attn) to approximate standard self-attention patterns effectively.\n",
      "- **Efficiency:** Both Lora and LongLoRA aim to reduce computational costs and training time, but LongLoRA specifically targets handling significantly larger context lengths.\n",
      "- **Integration:** LongLoRA integrates components like Flash-Attention2 and DeepSpeed to optimize training efficiency, enhancing its capabilities for long-context extension compared to Lora.\n",
      "\n",
      "In summary, while Lora focuses on efficient adaptation of pre-trained models, LongLoRA specializes in extending context length efficiently for large language models, offering enhanced question-answering abilities on long-context benchmarks.\n",
      "assistant: **Lora:**\n",
      "Lora, or Low-Rank Adaptation, is a method used for adapting large-scale pre-trained language models to specific tasks or domains. It involves introducing trainable rank decomposition matrices into each layer of the Transformer architecture while freezing the pre-trained model weights. This approach significantly reduces the number of trainable parameters for downstream tasks, making it more efficient and cost-effective to fine-tune large models like GPT-3 with a high number of parameters. Lora allows for the efficient adaptation of models to new tasks while preserving the learned knowledge from pre-training, ultimately improving model performance on various tasks with limited training data.\n",
      "\n",
      "**LongLoRA:**\n",
      "LongLoRA is a framework designed to efficiently extend the context length of large language models (LLMs) through supervised fine-tuning. It incorporates shifted sparse attention (S2-Attn) during training to approximate standard self-attention patterns effectively. LongLoRA aims to minimize computational costs and reduce training time while enabling pre-trained LLMs to handle significantly larger context lengths. By combining improved LoRA with S2-Attn, LongLoRA facilitates the fine-tuning of LLMs for long context extension, enhancing their question-answering abilities on long-context benchmarks. The framework also integrates components like Flash-Attention2 and DeepSpeed to optimize training efficiency, making it a valuable tool for extending model capabilities efficiently.\n",
      "\n",
      "**Comparison and Contrast:**\n",
      "- **Purpose:** Lora is focused on adapting pre-trained language models to specific tasks efficiently, while LongLoRA aims to extend the context length of large language models through supervised fine-tuning.\n",
      "- **Methodology:** Lora utilizes trainable rank decomposition matrices in each layer of the Transformer architecture, while LongLoRA incorporates shifted sparse attention (S2-Attn) to approximate standard self-attention patterns effectively.\n",
      "- **Efficiency:** Both Lora and LongLoRA aim to reduce computational costs and training time, but LongLoRA specifically targets handling significantly larger context lengths.\n",
      "- **Integration:** LongLoRA integrates components like Flash-Attention2 and DeepSpeed to optimize training efficiency, enhancing its capabilities for long-context extension compared to Lora.\n",
      "\n",
      "In summary, while Lora focuses on efficient adaptation of pre-trained models, LongLoRA specializes in extending context length efficiently for large language models, offering enhanced question-answering abilities on long-context benchmarks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Explain to me what is the Lora and why it's being used.\"\n",
    "    \"Explain to me what is LongLoRA and why it's being used.\"\n",
    "    \"Compare and contract LongLoRA and Lora.\"\n",
    ")\n",
    "\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
