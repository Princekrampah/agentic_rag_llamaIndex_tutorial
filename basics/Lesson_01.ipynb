{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"./datasets/lora_paper.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 2\n",
      "file_name: lora_paper.pdf\n",
      "file_path: datasets/lora_paper.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 1609513\n",
      "creation_date: 2024-05-12\n",
      "last_modified_date: 2024-05-12\n",
      "\n",
      "often introduce inference latency (Houlsby et al., 2019; Rebufﬁ et al., 2017) by extending model\n",
      "depth or reduce the model’s usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\n",
      "bardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\n",
      "match the ﬁne-tuning baselines, posing a trade-off between efﬁciency and model quality.\n",
      "We take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\n",
      "over-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\n",
      "change in weights during model adaptation also has a low “intrinsic rank”, leading to our proposed\n",
      "Low-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\n",
      "network indirectly by optimizing rank decomposition matrices of the dense layers’ change during\n",
      "adaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
      "175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufﬁces even\n",
      "when the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efﬁcient.\n",
      "LoRA possesses several key advantages.\n",
      "• A pre-trained model can be shared and used to build many small LoRA modules for dif-\n",
      "ferent tasks. We can freeze the shared model and efﬁciently switch tasks by replacing the\n",
      "matricesAandBin Figure 1, reducing the storage requirement and task-switching over-\n",
      "head signiﬁcantly.\n",
      "• LoRA makes training more efﬁcient and lowers the hardware barrier to entry by up to 3\n",
      "times when using adaptive optimizers since we do not need to calculate the gradients or\n",
      "maintain the optimizer states for most parameters. Instead, we only optimize the injected,\n",
      "much smaller low-rank matrices.\n",
      "• Our simple linear design allows us to merge the trainable matrices with the frozen weights\n",
      "when deployed, introducing no inference latency compared to a fully ﬁne-tuned model, by\n",
      "construction.\n",
      "• LoRA is orthogonal to many prior methods and can be combined with many of them, such\n",
      "as preﬁx-tuning. We provide an example in Appendix E.\n",
      "Terminologies and Conventions We make frequent references to the Transformer architecture\n",
      "and use the conventional terminologies for its dimensions. We call the input and output di-\n",
      "mension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\n",
      "query/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\n",
      "trained weight matrix and ∆Wits accumulated gradient update during adaptation. We use rto\n",
      "denote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\n",
      "Brown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\n",
      "optimization and use a Transformer MLP feedforward dimension dffn= 4×dmodel .\n",
      "2 P ROBLEM STATEMENT\n",
      "While our proposal is agnostic to training objective, we focus on language modeling as our motivat-\n",
      "ing use case. Below is a brief description of the language modeling problem and, in particular, the\n",
      "maximization of conditional probabilities given a task-speciﬁc prompt.\n",
      "Suppose we are given a pre-trained autoregressive language model PΦ(y|x)parametrized by Φ.\n",
      "For instance, PΦ(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\n",
      "et al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\n",
      "pre-trained model to downstream conditional text generation tasks, such as summarization, machine\n",
      "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\n",
      "represented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\n",
      "yiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\n",
      "corresponding SQL command; for summarization, xiis the content of an article and yiits summary.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "node_metadata = nodes[1].get_content(metadata_mode=True)\n",
    "print(str(node_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLM And Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embedding = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes=nodes)\n",
    "vecto_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_model=\"tree_summary\",\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vecto_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarizing of the lora paper.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "vecto_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context related to the lora paper.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_egine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[summary_tool, vecto_tool],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarizing of the lora paper..\n",
      "\u001b[0mThe LoRA paper introduces a novel adaptation strategy called Low-Rank Adaptation (LoRA) for large language models. LoRA aims to address the challenges of fine-tuning large models by freezing pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture. This approach significantly reduces the number of trainable parameters for downstream tasks, leading to improved efficiency in terms of memory requirements and training throughput. LoRA has been shown to perform on-par or better than traditional fine-tuning methods on various language models like RoBERTa, DeBERTa, GPT-2, and GPT-3, while also avoiding additional inference latency. The paper provides empirical investigations into rank-deficiency in language model adaptation and offers a package for integrating LoRA with PyTorch models, along with implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2.\n"
     ]
    }
   ],
   "source": [
    "response = query_egine.query(\"What is the lora paper about?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Retrieving specific context related to the lora paper would provide information on the evaluation datasets used..\n",
      "\u001b[0mThe evaluation datasets used in the LoRA paper include MNLI, STS-B, WikiSQL, SAMSum, E2E NLG Challenge, DART, and WebNLG.\n"
     ]
    }
   ],
   "source": [
    "response = query_egine.query(\"What eval datasets where used in the lora paper?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNLI-\n",
      "ndescribes a subset with ntraining examples. We evaluate with the full validation set. LoRA\n",
      "performs exhibits favorable sample-efﬁciency compared to other methods, including ﬁne-tuning.\n",
      "To be concrete, let the singular values of Ui⊤\n",
      "AUj\n",
      "Bto beσ1,σ2,···,σpwherep= min{i,j}. We\n",
      "know that the Projection Metric Ham & Lee (2008) is deﬁned as:\n",
      "d(Ui\n",
      "A,Uj\n",
      "B) =√p−p∑\n",
      "i=1σ2\n",
      "i∈[0,√p]\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
